{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+wNG8MYfhBMFPPQid7O2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablocosta/deleteRetreaveGenerate/blob/master/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn3Tfkb_kZa2",
        "outputId": "552f1997-a8dd-4201-c0f4-93d592537596"
      },
      "source": [
        "!git clone https://github.com/pablocosta/deleteRetreaveGenerate\n",
        "%cd deleteRetreaveGenerate\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deleteRetreaveGenerate'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 150 (delta 68), reused 99 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (150/150), 96.42 MiB | 25.68 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "/content/deleteRetreaveGenerate/deleteRetreaveGenerate/deleteRetreaveGenerate\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.16.4)\n",
            "Requirement already satisfied: Pillow==8.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (8.1.1)\n",
            "Requirement already satisfied: protobuf==3.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
            "Requirement already satisfied: tensorboardX==1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.7)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.8.0->-r requirements.txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.3.0->-r requirements.txt (line 9)) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torchvision==0.3.0->-r requirements.txt (line 9)) (3.7.4.3)\n",
            "[Errno 2] No such file or directory: 'deleteRetreaveGenerate'\n",
            "/content/deleteRetreaveGenerate/deleteRetreaveGenerate/deleteRetreaveGenerate\n",
            "/content/deleteRetreaveGenerate/deleteRetreaveGenerate/deleteRetreaveGenerate/tools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD0Bn8bkFMI"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byqwAAOeA_zD"
      },
      "source": [
        "#run experiment\n",
        "import sys\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import src.evaluation as evaluation\n",
        "from src.cuda import CUDA\n",
        "import src.data as data\n",
        "import src.models as models\n",
        "\n",
        "\n",
        "overfit = False\n",
        "bleu = True\n",
        "config = json.load(open(\"./yelp_config.json\", 'r'))\n",
        "\n",
        "workingDir = config['data']['working_dir']\n",
        "\n",
        "if not os.path.exists(workingDir):\n",
        "    os.makedirs(workingDir)\n",
        "\n",
        "config_path = os.path.join(workingDir, 'config.json')\n",
        "if not os.path.exists(config_path):\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "# set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='%s/train_log' % workingDir,\n",
        ")\n",
        "\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger('').addHandler(console)\n",
        "\n",
        "logging.info('Reading data ...')\n",
        "src, tgt = data.read_nmt_data(\n",
        "    src=config['data']['src'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes']\n",
        ")\n",
        "\n",
        "srcTest, tgtTest = data.read_nmt_data(\n",
        "    src=config['data']['src_test'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt_test'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes'],\n",
        "    train_src=src,\n",
        "    train_tgt=tgt\n",
        ")\n",
        "logging.info('...done!')\n",
        "\n",
        "logging.info('...done!')\n",
        "\n",
        "#model configs\n",
        "\n",
        "batchSize    = config['data']['batch_size']\n",
        "maxLength    = config['data']['max_len']\n",
        "srcVocabSize = len(src['tok2id'])\n",
        "tgtVocabSize = len(tgt['tok2id'])\n",
        "\n",
        "weightMask                         = torch.ones(tgtVocabSize)\n",
        "weightMask[tgt['tok2id']['<pad>']] = 0\n",
        "lossCriterion                      = nn.CrossEntropyLoss(weight=weightMask)\n",
        "\n",
        "if CUDA:\n",
        "    weightMask    = weightMask.cuda()\n",
        "    lossCriterion = lossCriterion.cuda()\n",
        "\n",
        "torch.manual_seed(config['training']['random_seed'])\n",
        "np.random.seed(config['training']['random_seed'])\n",
        "\n",
        "\n",
        "#model definition\n",
        "\n",
        "model = models.SeqModel(\n",
        "    srcVocabSize=srcVocabSize,\n",
        "    tgtVocabSize=tgtVocabSize,\n",
        "    padIdSrc=src['tok2id']['<pad>'],\n",
        "    padIdTgt=tgt['tok2id']['<pad>'],\n",
        "    batchSize=batchSize,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "logging.info('MODEL HAS %s params' %  model.countParams())\n",
        "model, startEpoch = models.attemptLoadModel(\n",
        "    model          = model,\n",
        "    checkpointDir  = workingDir)\n",
        "\n",
        "if CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "writer = SummaryWriter(workingDir)\n",
        "\n",
        "\n",
        "if config['training']['optimizer'] == 'adam':\n",
        "    lr        = config['training']['learning_rate']\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "elif config['training']['optimizer'] == 'sgd':\n",
        "    lr        = config['training']['learning_rate']\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "else:\n",
        "    raise NotImplementedError(\"Learning method not recommend for task\")\n",
        "\n",
        "epochLoss             = []\n",
        "startSinceLastReport  = time.time()\n",
        "wordsSinceLastReport  = 0\n",
        "lossesSinceLastReport = []\n",
        "bestMetric            = 0.0\n",
        "bestEpoch             = 0\n",
        "curMetric             = 0.0 # log perplexity or BLEU\n",
        "numExamples           = min(len(src['content']), len(tgt['content']))\n",
        "numBatches            = numExamples / batchSize\n",
        "\n",
        "\n",
        "\n",
        "STEP = 0\n",
        "for epoch in range(startEpoch, config['training']['epochs']):\n",
        "    if curMetric > bestMetric:\n",
        "        # rm old checkpoint\n",
        "        for ckpt_path in glob.glob(workingDir + '/model.*'):\n",
        "            os.system(\"rm %s\" % ckpt_path)\n",
        "        # replace with new checkpoint\n",
        "        torch.save(model.state_dict(), workingDir + '/model.%s.ckpt' % epoch)\n",
        "\n",
        "        bestMetric = curMetric\n",
        "        bestEpoch  = epoch - 1\n",
        "\n",
        "    losses = []\n",
        "    for i in range(0, numExamples, batchSize):\n",
        "\n",
        "        if overfit:\n",
        "            i = 50\n",
        "\n",
        "        batchIdx = i / batchSize\n",
        "        \n",
        "        inputContent, inputAux, outPut = data.minibatch(\n",
        "            src, tgt, i, batchSize, maxLength, config['model']['model_type']\n",
        "            )\n",
        "        \n",
        "        inputLinesSrc, _, srcLens, srcMask, _ = inputContent\n",
        "        inputIdsAux, _, auxLens, auxMask, _ = inputAux\n",
        "        inputLinesTgt, outputLinesTgt, _, _, _ = outPut\n",
        "        \n",
        "        decoderLogit, decoderProbs = model(inputLinesSrc, inputLinesTgt, srcMask, srcLens,\n",
        "            inputIdsAux, auxLens, auxMask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = lossCriterion(\n",
        "            decoderLogit.contiguous().view(-1, tgtVocabSize), outputLinesTgt.view(-1)\n",
        "        )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        lossesSinceLastReport.append(loss.item())\n",
        "        epochLoss.append(loss.item())\n",
        "        loss.backward()\n",
        "        norm = nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_norm'])\n",
        "\n",
        "        writer.add_scalar('stats/grad_norm', norm, STEP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if overfit or batchIdx % config['training']['batches_per_report'] == 0:\n",
        "\n",
        "            s = float(time.time() - startSinceLastReport)\n",
        "            eps = (batchSize * config['training']['batches_per_report']) / s\n",
        "            avgLoss = np.mean(lossesSinceLastReport)\n",
        "            info = (epoch, batchIdx, numBatches, eps, avgLoss, curMetric)\n",
        "            writer.add_scalar('stats/EPS', eps, STEP)\n",
        "            writer.add_scalar('stats/loss', avgLoss, STEP)\n",
        "            logging.info('EPOCH: %s ITER: %s/%s EPS: %.2f LOSS: %.4f METRIC: %.4f' % info)\n",
        "            startSinceLastReport = time.time()\n",
        "            wordsSinceLastReport = 0\n",
        "            lossesSinceLastReport = []\n",
        "\n",
        "\n",
        "        STEP += 1\n",
        "    if overfit:\n",
        "        continue\n",
        "\n",
        "    logging.info('EPOCH %s COMPLETE. EVALUATING...' % epoch)\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    \n",
        "    devLoss = evaluation.evaluateLpp(model, srcTest, tgtTest, config)\n",
        "\n",
        "    writer.add_scalar('eval/loss', devLoss, epoch)\n",
        "\n",
        "    if bleu and epoch >= config['training'].get('inference_start_epoch', 1):\n",
        "        curMetric, editDistance, inputs, preds, golds, auxs = evaluation.inferenceMetrics(\n",
        "            model, srcTest, tgtTest, config)\n",
        "\n",
        "        with open(workingDir + '/auxs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(auxs) + '\\n')\n",
        "        with open(workingDir + '/inputs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(inputs) + '\\n')\n",
        "        with open(workingDir + '/preds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(preds) + '\\n')\n",
        "        with open(workingDir + '/golds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(golds) + '\\n')\n",
        "\n",
        "        writer.add_scalar('eval/edit_distance', editDistance, epoch)\n",
        "        writer.add_scalar('eval/bleu', curMetric, epoch)\n",
        "\n",
        "    else:\n",
        "        cur_metric = devLoss\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    logging.info('METRIC: %s. TIME: %.2fs CHECKPOINTING...' % (\n",
        "        curMetric, (time.time() - start)))\n",
        "    avgLoss = np.mean(epochLoss)\n",
        "    epochLoss = []\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLDTpy3D-VeO"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir working_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXryQXZxLS2H"
      },
      "source": [
        "!rm -r ./working_dir/*"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}