{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBdV+Nr9COX+nQOizAalLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablocosta/deleteRetreaveGenerate/blob/master/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGBc0WrFAMGs",
        "outputId": "c53c9562-b9a9-4203-ff2d-22928f2ad7b4"
      },
      "source": [
        "!git clone https://github.com/pablocosta/deleteRetreaveGenerate\n",
        "%cd deleteRetreaveGenerate\n",
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deleteRetreaveGenerate'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 53 (delta 14), reused 43 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "/content/deleteRetreaveGenerate\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.5.3)\n",
            "Collecting joblib==0.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 14.3MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 306kB/s \n",
            "\u001b[?25hCollecting Pillow==8.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/f9/5173fdbba404815d5109067ecde640dab908f4cd22b2c9de7bbedee46d67/Pillow-8.1.1-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 44.1MB/s \n",
            "\u001b[?25hCollecting protobuf==3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/86/9f6123c4c6f481862f286dbe13aa2e97bdedd7662f5fc3033c1a41f32f88/protobuf-3.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting tensorboardX==1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 28.4MB/s \n",
            "\u001b[?25hCollecting torchvision==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/9b/208f48d5a5013bdb0c27a84a02df4fcf5fd24ab5902667c11e554a12b681/torchvision-0.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.8.0->-r requirements.txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.3.0->-r requirements.txt (line 9)) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torchvision==0.3.0->-r requirements.txt (line 9)) (3.7.4.3)\n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement protobuf>=3.9.2, but you'll have protobuf 3.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.1 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement protobuf>=3.12.0, but you'll have protobuf 3.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cupy-cuda101 9.1.0 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: joblib, numpy, Pillow, six, protobuf, tensorboardX, torchvision\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed Pillow-8.1.1 joblib-0.13.2 numpy-1.16.4 protobuf-3.8.0 six-1.12.0 tensorboardX-1.7 torchvision-0.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "byqwAAOeA_zD",
        "outputId": "777a33d2-a479-4805-c16b-01a8c8f99363"
      },
      "source": [
        "#run experiment\n",
        "import sys\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import src.evaluation as evaluation\n",
        "from src.cuda import CUDA\n",
        "import src.data as data\n",
        "import src.models as models\n",
        "\n",
        "\n",
        "overfit = False\n",
        "\n",
        "config = json.load(open(\"./yelp_config.json\", 'r'))\n",
        "\n",
        "working_dir = config['data']['working_dir']\n",
        "\n",
        "if not os.path.exists(working_dir):\n",
        "    os.makedirs(working_dir)\n",
        "\n",
        "config_path = os.path.join(working_dir, 'config.json')\n",
        "if not os.path.exists(config_path):\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "# set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='%s/train_log' % working_dir,\n",
        ")\n",
        "\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger('').addHandler(console)\n",
        "\n",
        "logging.info('Reading data ...')\n",
        "src, tgt = data.read_nmt_data(\n",
        "    src=config['data']['src'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes']\n",
        ")\n",
        "\n",
        "src_test, tgt_test = data.read_nmt_data(\n",
        "    src=config['data']['src_test'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt_test'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes'],\n",
        "    train_src=src,\n",
        "    train_tgt=tgt\n",
        ")\n",
        "logging.info('...done!')\n",
        "\n",
        "\n",
        "batch_size = config['data']['batch_size']\n",
        "max_length = config['data']['max_len']\n",
        "src_vocab_size = len(src['tok2id'])\n",
        "tgt_vocab_size = len(tgt['tok2id'])\n",
        "\n",
        "\n",
        "weight_mask = torch.ones(tgt_vocab_size)\n",
        "weight_mask[tgt['tok2id']['<pad>']] = 0\n",
        "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)\n",
        "if CUDA:\n",
        "    weight_mask = weight_mask.cuda()\n",
        "    loss_criterion = loss_criterion.cuda()\n",
        "\n",
        "torch.manual_seed(config['training']['random_seed'])\n",
        "np.random.seed(config['training']['random_seed'])\n",
        "\n",
        "model = models.SeqModel(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    pad_id_src=src['tok2id']['<pad>'],\n",
        "    pad_id_tgt=tgt['tok2id']['<pad>'],\n",
        "    config=config\n",
        ")\n",
        "\n",
        "logging.info('MODEL HAS %s params' %  model.count_params())\n",
        "model, start_epoch = models.attempt_load_model(\n",
        "    model=model,\n",
        "    checkpoint_dir=working_dir)\n",
        "if CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "writer = SummaryWriter(working_dir)\n",
        "\n",
        "\n",
        "if config['training']['optimizer'] == 'adam':\n",
        "    lr = config['training']['learning_rate']\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "elif config['training']['optimizer'] == 'sgd':\n",
        "    lr = config['training']['learning_rate']\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "else:\n",
        "    raise NotImplementedError(\"Learning method not recommend for task\")\n",
        "\n",
        "epoch_loss = []\n",
        "start_since_last_report = time.time()\n",
        "words_since_last_report = 0\n",
        "losses_since_last_report = []\n",
        "best_metric = 0.0\n",
        "best_epoch = 0\n",
        "cur_metric = 0.0 # log perplexity or BLEU\n",
        "num_examples = min(len(src['content']), len(tgt['content']))\n",
        "num_batches = num_examples / batch_size\n",
        "\n",
        "STEP = 0\n",
        "for epoch in range(start_epoch, config['training']['epochs']):\n",
        "    if cur_metric > best_metric:\n",
        "        # rm old checkpoint\n",
        "        for ckpt_path in glob.glob(working_dir + '/model.*'):\n",
        "            os.system(\"rm %s\" % ckpt_path)\n",
        "        # replace with new checkpoint\n",
        "        torch.save(model.state_dict(), working_dir + '/model.%s.ckpt' % epoch)\n",
        "\n",
        "        best_metric = cur_metric\n",
        "        best_epoch = epoch - 1\n",
        "\n",
        "    losses = []\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "\n",
        "        if overfit:\n",
        "            i = 50\n",
        "\n",
        "        batch_idx = i / batch_size\n",
        "\n",
        "        input_content, input_aux, output = data.minibatch(\n",
        "            src, tgt, i, batch_size, max_length, config['model']['model_type'])\n",
        "        input_lines_src, _, srclens, srcmask, _ = input_content\n",
        "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
        "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
        "        \n",
        "        decoder_logit, decoder_probs = model(\n",
        "            input_lines_src, input_lines_tgt, srcmask, srclens,\n",
        "            input_ids_aux, auxlens, auxmask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_criterion(\n",
        "            decoder_logit.contiguous().view(-1, tgt_vocab_size),\n",
        "            output_lines_tgt.view(-1)\n",
        "        )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        losses_since_last_report.append(loss.item())\n",
        "        epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        norm = nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_norm'])\n",
        "\n",
        "        writer.add_scalar('stats/grad_norm', norm, STEP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if overfit or batch_idx % config['training']['batches_per_report'] == 0:\n",
        "\n",
        "            s = float(time.time() - start_since_last_report)\n",
        "            eps = (batch_size * config['training']['batches_per_report']) / s\n",
        "            avg_loss = np.mean(losses_since_last_report)\n",
        "            info = (epoch, batch_idx, num_batches, eps, avg_loss, cur_metric)\n",
        "            writer.add_scalar('stats/EPS', eps, STEP)\n",
        "            writer.add_scalar('stats/loss', avg_loss, STEP)\n",
        "            logging.info('EPOCH: %s ITER: %s/%s EPS: %.2f LOSS: %.4f METRIC: %.4f' % info)\n",
        "            start_since_last_report = time.time()\n",
        "            words_since_last_report = 0\n",
        "            losses_since_last_report = []\n",
        "\n",
        "        # NO SAMPLING!! because weird train-vs-test data stuff would be a pain\n",
        "        STEP += 1\n",
        "    if overfit:\n",
        "        continue\n",
        "\n",
        "    logging.info('EPOCH %s COMPLETE. EVALUATING...' % epoch)\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    dev_loss = evaluation.evaluate_lpp(\n",
        "            model, src_test, tgt_test, config)\n",
        "\n",
        "    writer.add_scalar('eval/loss', dev_loss, epoch)\n",
        "\n",
        "    if args.bleu and epoch >= config['training'].get('inference_start_epoch', 1):\n",
        "        cur_metric, edit_distance, inputs, preds, golds, auxs = evaluation.inference_metrics(\n",
        "            model, src_test, tgt_test, config)\n",
        "\n",
        "        with open(working_dir + '/auxs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(auxs) + '\\n')\n",
        "        with open(working_dir + '/inputs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(inputs) + '\\n')\n",
        "        with open(working_dir + '/preds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(preds) + '\\n')\n",
        "        with open(working_dir + '/golds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(golds) + '\\n')\n",
        "\n",
        "        writer.add_scalar('eval/edit_distance', edit_distance, epoch)\n",
        "        writer.add_scalar('eval/bleu', cur_metric, epoch)\n",
        "\n",
        "    else:\n",
        "        cur_metric = dev_loss\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    logging.info('METRIC: %s. TIME: %.2fs CHECKPOINTING...' % (\n",
        "        cur_metric, (time.time() - start)))\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    epoch_loss = []\n",
        "\n",
        "writer.close()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-18 16:22:34,829 - INFO - Reading data ...\n",
            "2021-06-18 16:23:03,529 - INFO - ...done!\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "2021-06-18 16:23:09,918 - INFO - MODEL HAS 9181445 params\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load from working_dir/model.1.ckpt sucessful!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-18 16:23:10,212 - INFO - EPOCH: 2 ITER: 0.0/692.2578125 EPS: 262923.59 LOSS: 4.7783 METRIC: 0.0000\n",
            "2021-06-18 16:23:16,074 - INFO - EPOCH: 2 ITER: 200.0/692.2578125 EPS: 8735.59 LOSS: 4.4013 METRIC: 0.0000\n",
            "2021-06-18 16:23:21,923 - INFO - EPOCH: 2 ITER: 400.0/692.2578125 EPS: 8756.63 LOSS: 4.1055 METRIC: 0.0000\n",
            "2021-06-18 16:23:27,740 - INFO - EPOCH: 2 ITER: 600.0/692.2578125 EPS: 8804.87 LOSS: 3.9167 METRIC: 0.0000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2657518fd076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# NO SAMPLING!! because weird train-vs-test data stuff would be a pain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mSTEP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    }
  ]
}