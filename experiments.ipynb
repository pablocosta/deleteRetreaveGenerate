{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNExo2FS7LrbRMny59TMyaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablocosta/deleteRetreaveGenerate/blob/master/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGBc0WrFAMGs",
        "outputId": "59de8b2f-e0df-479f-b3cc-4332dddb5cab"
      },
      "source": [
        "!git clone https://github.com/pablocosta/deleteRetreaveGenerate\n",
        "%cd deleteRetreaveGenerate\n",
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deleteRetreaveGenerate' already exists and is not an empty directory.\n",
            "/content/deleteRetreaveGenerate\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.16.4)\n",
            "Requirement already satisfied: Pillow==8.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (8.1.1)\n",
            "Requirement already satisfied: protobuf==3.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
            "Requirement already satisfied: tensorboardX==1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.7)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.8.0->-r requirements.txt (line 5)) (57.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.3.0->-r requirements.txt (line 9)) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torchvision==0.3.0->-r requirements.txt (line 9)) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "byqwAAOeA_zD",
        "outputId": "0a7030df-788b-477e-b6d4-773df7a5b68f"
      },
      "source": [
        "#run experiment\n",
        "import sys\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import src.evaluation as evaluation\n",
        "from src.cuda import CUDA\n",
        "import src.data as data\n",
        "import src.models as models\n",
        "\n",
        "\n",
        "overfit = False\n",
        "bleu = True\n",
        "config = json.load(open(\"./yelp_config.json\", 'r'))\n",
        "\n",
        "workingDir = config['data']['working_dir']\n",
        "\n",
        "if not os.path.exists(workingDir):\n",
        "    os.makedirs(workingDir)\n",
        "\n",
        "config_path = os.path.join(workingDir, 'config.json')\n",
        "if not os.path.exists(config_path):\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "# set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='%s/train_log' % workingDir,\n",
        ")\n",
        "\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger('').addHandler(console)\n",
        "\n",
        "logging.info('Reading data ...')\n",
        "src, tgt = data.read_nmt_data(\n",
        "    src=config['data']['src'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes']\n",
        ")\n",
        "\n",
        "srcTest, tgtTest = data.read_nmt_data(\n",
        "    src=config['data']['src_test'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt_test'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes'],\n",
        "    train_src=src,\n",
        "    train_tgt=tgt\n",
        ")\n",
        "logging.info('...done!')\n",
        "\n",
        "logging.info('...done!')\n",
        "\n",
        "#model configs\n",
        "\n",
        "batchSize    = config['data']['batch_size']\n",
        "maxLength    = config['data']['max_len']\n",
        "srcVocabSize = len(src['tok2id'])\n",
        "tgtVocabSize = len(tgt['tok2id'])\n",
        "\n",
        "weightMask                         = torch.ones(tgtVocabSize)\n",
        "weightMask[tgt['tok2id']['<pad>']] = 0\n",
        "lossCriterion                      = nn.CrossEntropyLoss(weight=weightMask)\n",
        "\n",
        "if CUDA:\n",
        "    weightMask    = weightMask.cuda()\n",
        "    lossCriterion = lossCriterion.cuda()\n",
        "\n",
        "torch.manual_seed(config['training']['random_seed'])\n",
        "np.random.seed(config['training']['random_seed'])\n",
        "\n",
        "\n",
        "#model definition\n",
        "\n",
        "model = models.SeqModel(\n",
        "    srcVocabSize=srcVocabSize,\n",
        "    tgtVocabSize=tgtVocabSize,\n",
        "    padIdSrc=src['tok2id']['<pad>'],\n",
        "    padIdTgt=tgt['tok2id']['<pad>'],\n",
        "    batchSize=batchSize,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "logging.info('MODEL HAS %s params' %  model.countParams())\n",
        "model, startEpoch = models.attemptLoadModel(\n",
        "    model          = model,\n",
        "    checkpointDir  = workingDir)\n",
        "\n",
        "if CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "writer = SummaryWriter(workingDir)\n",
        "\n",
        "\n",
        "if config['training']['optimizer'] == 'adam':\n",
        "    lr        = config['training']['learning_rate']\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "elif config['training']['optimizer'] == 'sgd':\n",
        "    lr        = config['training']['learning_rate']\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "else:\n",
        "    raise NotImplementedError(\"Learning method not recommend for task\")\n",
        "\n",
        "epochLoss             = []\n",
        "startSinceLastReport  = time.time()\n",
        "wordsSinceLastReport  = 0\n",
        "lossesSinceLastReport = []\n",
        "bestMetric            = 0.0\n",
        "bestEpoch             = 0\n",
        "curMetric             = 0.0 # log perplexity or BLEU\n",
        "numExamples           = min(len(src['content']), len(tgt['content']))\n",
        "numBatches            = numExamples / batchSize\n",
        "\n",
        "\n",
        "\n",
        "STEP = 0\n",
        "for epoch in range(startEpoch, config['training']['epochs']):\n",
        "    if curMetric > bestMetric:\n",
        "        # rm old checkpoint\n",
        "        for ckpt_path in glob.glob(workingDir + '/model.*'):\n",
        "            os.system(\"rm %s\" % ckpt_path)\n",
        "        # replace with new checkpoint\n",
        "        torch.save(model.state_dict(), workingDir + '/model.%s.ckpt' % epoch)\n",
        "\n",
        "        bestMetric = curMetric\n",
        "        bestEpoch  = epoch - 1\n",
        "\n",
        "    losses = []\n",
        "    for i in range(0, numExamples, batchSize):\n",
        "\n",
        "        if overfit:\n",
        "            i = 50\n",
        "\n",
        "        batchIdx = i / batchSize\n",
        "        \n",
        "        inputContent, inputAux, outPut = data.minibatch(\n",
        "            src, tgt, i, batchSize, maxLength, config['model']['model_type']\n",
        "            )\n",
        "        \n",
        "        inputLinesSrc, _, srcLens, srcMask, _ = inputContent\n",
        "        inputIdsAux, _, auxLens, auxMask, _ = inputAux\n",
        "        inputLinesTgt, outputLinesTgt, _, _, _ = outPut\n",
        "        \n",
        "        decoderLogit, decoderProbs = model(inputLinesSrc, inputLinesTgt, srcMask, srcLens,\n",
        "            inputIdsAux, auxLens, auxMask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = lossCriterion(\n",
        "            decoderLogit.contiguous().view(-1, tgtVocabSize), outputLinesTgt.view(-1)\n",
        "        )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        lossesSinceLastReport.append(loss.item())\n",
        "        epochLoss.append(loss.item())\n",
        "        loss.backward()\n",
        "        norm = nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_norm'])\n",
        "\n",
        "        writer.add_scalar('stats/grad_norm', norm, STEP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if overfit or batchIdx % config['training']['batches_per_report'] == 0:\n",
        "\n",
        "            s = float(time.time() - startSinceLastReport)\n",
        "            eps = (batchSize * config['training']['batches_per_report']) / s\n",
        "            avgLoss = np.mean(lossesSinceLastReport)\n",
        "            info = (epoch, batchIdx, numBatches, eps, avgLoss, curMetric)\n",
        "            writer.add_scalar('stats/EPS', eps, STEP)\n",
        "            writer.add_scalar('stats/loss', avgLoss, STEP)\n",
        "            logging.info('EPOCH: %s ITER: %s/%s EPS: %.2f LOSS: %.4f METRIC: %.4f' % info)\n",
        "            startSinceLastReport = time.time()\n",
        "            wordsSinceLastReport = 0\n",
        "            lossesSinceLastReport = []\n",
        "\n",
        "\n",
        "        STEP += 1\n",
        "    if overfit:\n",
        "        continue\n",
        "\n",
        "    logging.info('EPOCH %s COMPLETE. EVALUATING...' % epoch)\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    \n",
        "    devLoss = evaluation.evaluateLpp(model, srcTest, tgtTest, config)\n",
        "\n",
        "    writer.add_scalar('eval/loss', devLoss, epoch)\n",
        "\n",
        "    if bleu and epoch >= config['training'].get('inference_start_epoch', 1):\n",
        "        curMetric, editDistance, inputs, preds, golds, auxs = evaluation.inferenceMetrics(\n",
        "            model, srcTest, tgtTest, config)\n",
        "\n",
        "        with open(workingDir + '/auxs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(auxs) + '\\n')\n",
        "        with open(workingDir + '/inputs.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(inputs) + '\\n')\n",
        "        with open(workingDir + '/preds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(preds) + '\\n')\n",
        "        with open(workingDir + '/golds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(golds) + '\\n')\n",
        "\n",
        "        writer.add_scalar('eval/edit_distance', editDistance, epoch)\n",
        "        writer.add_scalar('eval/bleu', curMetric, epoch)\n",
        "\n",
        "    else:\n",
        "        cur_metric = devLoss\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    logging.info('METRIC: %s. TIME: %.2fs CHECKPOINTING...' % (\n",
        "        curMetric, (time.time() - start)))\n",
        "    avgLoss = np.mean(epochLoss)\n",
        "    epochLoss = []\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-03 14:36:37,871 - INFO - Reading data ...\n",
            "2021-07-03 14:37:04,894 - INFO - ...done!\n",
            "2021-07-03 14:37:04,896 - INFO - ...done!\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "2021-07-03 14:37:07,296 - INFO - MODEL HAS 10364933 params\n",
            "2021-07-03 14:37:09,281 - INFO - EPOCH: 0 ITER: 0.0/692.2578125 EPS: 26072.22 LOSS: 9.1709 METRIC: 0.0000\n",
            "2021-07-03 14:44:46,495 - INFO - EPOCH: 0 ITER: 200.0/692.2578125 EPS: 111.98 LOSS: 5.7935 METRIC: 0.0000\n",
            "2021-07-03 14:52:26,259 - INFO - EPOCH: 0 ITER: 400.0/692.2578125 EPS: 111.36 LOSS: 5.1319 METRIC: 0.0000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ad2074fe417c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         inputContent, inputAux, outPut = data.minibatch(\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             )\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deleteRetreaveGenerate/src/data.py\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(src, tgt, idx, batch_size, max_len, model_type, is_test)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mout_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attribute'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tok2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 dist_measurer=out_dataset['dist_measurer'], sample_rate=0.1)\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'seq2seq'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deleteRetreaveGenerate/src/data.py\u001b[0m in \u001b[0;36mget_minibatch\u001b[0;34m(lines, tok2id, index, batch_size, max_len, sort, idx, dist_measurer, sample_rate)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdist_measurer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_measurer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deleteRetreaveGenerate/src/data.py\u001b[0m in \u001b[0;36msample_replace\u001b[0;34m(lines, dist_measurer, sample_rate, corpus_idx)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# top match is the current line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_measurer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deleteRetreaveGenerate/src/data.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, key_idx, n)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mscores_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# use the retrieved i to pick examples from the VALUE corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLDTpy3D-VeO"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir working_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXryQXZxLS2H"
      },
      "source": [
        "!rm -r ./working_dir/*"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}