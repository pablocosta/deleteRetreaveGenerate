{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exploratoryAnalisys.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLmHlobDQy3VnwsKIU+21L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablocosta/deleteRetreaveGenerate/blob/master/exploratoryAnalisys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8jfFS47r6vQ"
      },
      "source": [
        "# Experimentos iniciais - Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFLQBUe3sJHL",
        "outputId": "1615a841-2abb-46a0-dfe4-fc4afeb9f595"
      },
      "source": [
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "\n",
        "#mount and read data\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/sentimentTranslation/data/b2w.csv', sep=\";\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfy8JOEKNLo-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c12f1d7-ba54-46dc-c729-2bd6225b168d"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>submission_date</th>\n",
              "      <th>reviewer_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_name</th>\n",
              "      <th>product_brand</th>\n",
              "      <th>site_category_lv1</th>\n",
              "      <th>site_category_lv2</th>\n",
              "      <th>review_title</th>\n",
              "      <th>overall_rating</th>\n",
              "      <th>recommend_to_a_friend</th>\n",
              "      <th>review_text</th>\n",
              "      <th>reviewer_birth_year</th>\n",
              "      <th>reviewer_gender</th>\n",
              "      <th>reviewer_state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-01 00:11:28</td>\n",
              "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
              "      <td>132532965</td>\n",
              "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Informática</td>\n",
              "      <td>Notebook</td>\n",
              "      <td>Bom</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
              "      <td>1958.0</td>\n",
              "      <td>F</td>\n",
              "      <td>RJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-01-01 00:13:48</td>\n",
              "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
              "      <td>22562178</td>\n",
              "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Utilidades Domésticas</td>\n",
              "      <td>Copos, Taças e Canecas</td>\n",
              "      <td>Preço imbatível, ótima qualidade</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>M</td>\n",
              "      <td>SC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-01-01 00:26:02</td>\n",
              "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
              "      <td>113022329</td>\n",
              "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
              "      <td>philips walita</td>\n",
              "      <td>Eletroportáteis</td>\n",
              "      <td>Panela Elétrica</td>\n",
              "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
              "      <td>1984.0</td>\n",
              "      <td>M</td>\n",
              "      <td>SP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-01-01 00:35:54</td>\n",
              "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
              "      <td>113851581</td>\n",
              "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
              "      <td>roma jensen</td>\n",
              "      <td>Brinquedos</td>\n",
              "      <td>Veículos de Brinquedo</td>\n",
              "      <td>presente mais que desejado</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
              "      <td>1985.0</td>\n",
              "      <td>F</td>\n",
              "      <td>SP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-01-01 01:00:28</td>\n",
              "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
              "      <td>131788803</td>\n",
              "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
              "      <td>lg</td>\n",
              "      <td>TV e Home Theater</td>\n",
              "      <td>TV</td>\n",
              "      <td>Sem duvidas, excelente</td>\n",
              "      <td>5</td>\n",
              "      <td>Yes</td>\n",
              "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
              "      <td>1994.0</td>\n",
              "      <td>M</td>\n",
              "      <td>MG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132368</th>\n",
              "      <td>2018-05-31 23:30:50</td>\n",
              "      <td>15f20e95ff44163f3175aaf67a5ae4a94d5030b409e521...</td>\n",
              "      <td>17962233</td>\n",
              "      <td>Carregador De Pilha Sony + 4 Pilhas Aa 2500mah</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Câmeras e Filmadoras</td>\n",
              "      <td>Acessórios para Câmeras e Filmadoras</td>\n",
              "      <td>Ótimo produto!</td>\n",
              "      <td>5</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Vale muito, estou usando no controle do Xbox e...</td>\n",
              "      <td>1988.0</td>\n",
              "      <td>M</td>\n",
              "      <td>RS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132369</th>\n",
              "      <td>2018-05-31 23:42:25</td>\n",
              "      <td>def7cf9028b0673ab8bca3b1d06e085461fafb88cd48d9...</td>\n",
              "      <td>132631701</td>\n",
              "      <td>Mop Giratório Fit + Refil Extra - At Home</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Utilidades Domésticas</td>\n",
              "      <td>Material de Limpeza</td>\n",
              "      <td>Sensacional</td>\n",
              "      <td>5</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Prático e barato, super indico o produto para ...</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>F</td>\n",
              "      <td>SP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132370</th>\n",
              "      <td>2018-05-31 23:44:16</td>\n",
              "      <td>7bcbf542f5d7dd9a9a192a6805adba7a7a4c1ce3bf00df...</td>\n",
              "      <td>16095859</td>\n",
              "      <td>Fita Led 5m Rgb 3528 Siliconada Com 300 Leds C...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Automotivo</td>\n",
              "      <td>Iluminação</td>\n",
              "      <td>Ótimo produto</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Chegou antes do prazo previsto e corresponde a...</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>F</td>\n",
              "      <td>PR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132371</th>\n",
              "      <td>2018-05-31 23:46:48</td>\n",
              "      <td>e6fb0b19277d01c2a300c7837a105f3c369377e92f9c19...</td>\n",
              "      <td>6774907</td>\n",
              "      <td>Etiquetas Jurídicas Vade Mecum - Marca Fácil</td>\n",
              "      <td>marca facil</td>\n",
              "      <td>Papelaria</td>\n",
              "      <td>Material de Escritório</td>\n",
              "      <td>O produto não é bom.</td>\n",
              "      <td>1</td>\n",
              "      <td>No</td>\n",
              "      <td>Material fraco, poderia ser melhor. Ficou deve...</td>\n",
              "      <td>1991.0</td>\n",
              "      <td>M</td>\n",
              "      <td>RJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132372</th>\n",
              "      <td>2018-05-31 23:50:33</td>\n",
              "      <td>ea9467aa73271fb4f68c04f4dd4f4eef304d6ee85441fb...</td>\n",
              "      <td>114081902</td>\n",
              "      <td>Painel de Fotos Bee Colection Rue Bac (74x94x3...</td>\n",
              "      <td>kapos</td>\n",
              "      <td>Decoração</td>\n",
              "      <td>Painel de Fotos</td>\n",
              "      <td>Produto não entregue</td>\n",
              "      <td>1</td>\n",
              "      <td>No</td>\n",
              "      <td>Comprei esse produto, quando chegou estava com...</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>F</td>\n",
              "      <td>ES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>132373 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            submission_date  ... reviewer_state\n",
              "0       2018-01-01 00:11:28  ...             RJ\n",
              "1       2018-01-01 00:13:48  ...             SC\n",
              "2       2018-01-01 00:26:02  ...             SP\n",
              "3       2018-01-01 00:35:54  ...             SP\n",
              "4       2018-01-01 01:00:28  ...             MG\n",
              "...                     ...  ...            ...\n",
              "132368  2018-05-31 23:30:50  ...             RS\n",
              "132369  2018-05-31 23:42:25  ...             SP\n",
              "132370  2018-05-31 23:44:16  ...             PR\n",
              "132371  2018-05-31 23:46:48  ...             RJ\n",
              "132372  2018-05-31 23:50:33  ...             ES\n",
              "\n",
              "[132373 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V88iQ_ITxkNx"
      },
      "source": [
        "dfNew = pd.DataFrame()\n",
        "\n",
        "dfNew[\"X\"] = df[\"review_text\"]\n",
        "dfNew[\"Y\"] = df[\"recommend_to_a_friend\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Z21e7lxgD3",
        "outputId": "15b42c33-e21c-45d3-b615-48ec7321da25"
      },
      "source": [
        "#count data\n",
        "\n",
        "df[\"Polarity\"].value_counts()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "against    5603\n",
              "for        4115\n",
              "other       238\n",
              "Name: Polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKuglZbxzzr"
      },
      "source": [
        "#clean data\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def cleanData(x):\n",
        "  words = re.split(r'\\W+', x)  \n",
        "  stripped = [w.translate(table).lower() for w in words]\n",
        "  #tokens = word_tokenize(text, language=\"portuguese\")\n",
        "  # remove all tokens that are not alphabetic\n",
        "  #words = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  return \" \".join(stripped)\n",
        "\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: cleanData(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ37JoRg0BSW"
      },
      "source": [
        "dfAgainst  = df[df[\"Polarity\"] == \"against\"]\n",
        "dfInfavor  = df[df[\"Polarity\"] == \"for\"]\n",
        "dfCombined = pd.concat([dfAgainst, dfInfavor])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBPLCd_X_imL",
        "outputId": "3f2b9b92-1598-4478-e761-607fc51e8fe0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5603, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1iSe6Ln9xnw"
      },
      "source": [
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/corpus.txt\",\"w+\")\n",
        "\n",
        "for row in dfCombined.iterrows():\n",
        "\n",
        "  allFile.write(row[1][\"Text\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEoZ9mipKPg"
      },
      "source": [
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/pos.txt\",\"w+\")\n",
        "\n",
        "for row in dfInfavor.iterrows():\n",
        "  allFile.write(row[1][\"Text\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VCA7NkCpKSP"
      },
      "source": [
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/neg.txt\",\"w+\")\n",
        "\n",
        "for row in dfAgainst.iterrows():\n",
        "  allFile.write(row[1][\"Text\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOmQ4yDlpKVY"
      },
      "source": [
        "#train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dfInfavor[\"Text\"], dfInfavor[\"Target\"], test_size=0.2)\n",
        "\n",
        "#read data dataset and store\n",
        "fileTrain = open(r\"/content/drive/MyDrive/sentimentTranslation/data/pos.train.txt\",\"w+\")\n",
        "fileTeste = open(r\"/content/drive/MyDrive/sentimentTranslation/data/pos.test.txt\",\"w+\")\n",
        "\n",
        "for xT in X_train:\n",
        "  fileTrain.write(xT+\"\\n\")\n",
        "\n",
        "\n",
        "for xTe in X_test:\n",
        "  fileTeste.write(xTe+\"\\n\")\n",
        "  \n",
        "fileTrain.close()\n",
        "fileTeste.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DJRJp-CAy3W",
        "outputId": "30bc2521-458d-4262-b8eb-f776bf548d5f"
      },
      "source": [
        "dfAgainst.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5603, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KEsQS8RA3Wu",
        "outputId": "b68b7ecb-8e1d-4997-e641-7a8e8a954ec5"
      },
      "source": [
        "dfInfavor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4115, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qojxqsEuAxoR"
      },
      "source": [
        "fileTrain = open(r\"/content/drive/MyDrive/sentimentTranslation/data/neg.train.txt\",\"w+\")\n",
        "fileTeste = open(r\"/content/drive/MyDrive/sentimentTranslation/data/neg.test.txt\",\"w+\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dfAgainst[\"Text\"], dfAgainst[\"Target\"], test_size=0.2)\n",
        "\n",
        "for xT in X_train:\n",
        "  fileTrain.write(xT+\"\\n\")\n",
        "\n",
        "\n",
        "for xTe in X_test:\n",
        "  fileTeste.write(xTe+\"\\n\")\n",
        "\n",
        "fileTrain.close()\n",
        "fileTeste.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZKndHmOnskj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH8l06manuUw"
      },
      "source": [
        "#Data-set keaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkeQ_3DToCOE",
        "outputId": "36d4569a-44fd-4f5d-b421-56c3a149da02"
      },
      "source": [
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#mount and read data\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_data():\n",
        "    no_theme = pd.read_csv(\n",
        "        '/content/drive/MyDrive/sentimentTranslation/data/NoThemeTweets.csv', \n",
        "        index_col=0)\n",
        "    # the `type` column will be important in the future to stratify the splits\n",
        "    no_theme['type'] = 'no_theme-'\n",
        "\n",
        "    with_theme = pd.read_csv(\n",
        "        '/content/drive/MyDrive/sentimentTranslation/data/TweetsWithTheme.csv', \n",
        "        index_col=0)\n",
        "    with_theme['type'] = 'with_theme-'\n",
        "\n",
        "    data = pd.concat([no_theme, with_theme])\n",
        "    data['type'] = data['type'] + data['sentiment']\n",
        "    # Remove duplicate tweets\n",
        "    data = data[~data.index.duplicated(keep='first')]\n",
        "    \n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "data\n",
        "\n",
        "\n",
        "def create_splits(data):\n",
        "    test_validation_size = int(0.2*data.shape[0])\n",
        "    train_validation, test = train_test_split(data, test_size=test_validation_size, random_state=42, stratify=data['type'])\n",
        "   \n",
        "    return train_validation, test\n",
        "train, test = create_splits(data)\n",
        "print('Training samples:  ', train.shape[0])\n",
        "print('Test samples:      ', test.shape[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training samples:   668853\n",
            "Test samples:       167213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "Q78vNs2YwoaY",
        "outputId": "19595770-51ce-4022-9d7b-b8d37a69c8dc"
      },
      "source": [
        "train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_date</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>query_used</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1047327129678110721</th>\n",
              "      <td>@afabianepereira além disso, o macro sempre va...</td>\n",
              "      <td>Wed Oct 03 03:27:00 +0000 2018</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>:)</td>\n",
              "      <td>no_theme-Positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038220551863627777</th>\n",
              "      <td>aaaaaaa firmino :))))</td>\n",
              "      <td>Sat Sep 08 00:20:43 +0000 2018</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>:)</td>\n",
              "      <td>no_theme-Positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042927414794440704</th>\n",
              "      <td>@smtferreira não me chateia desse jeito :(((</td>\n",
              "      <td>Fri Sep 21 00:04:06 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036605417290383360</th>\n",
              "      <td>@ddlnopromises adeus meta fitness :(</td>\n",
              "      <td>Mon Sep 03 13:22:45 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042021132210200576</th>\n",
              "      <td>@mateus_brando8 @percosk Vc disse \"generaliza ...</td>\n",
              "      <td>Tue Sep 18 12:02:52 +0000 2018</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>:)</td>\n",
              "      <td>no_theme-Positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044415947983998977</th>\n",
              "      <td>eu te amo muito muito muito :( https://t.co/um...</td>\n",
              "      <td>Tue Sep 25 02:39:00 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043731778488406016</th>\n",
              "      <td>poxa nao estou na lista :( https://t.co/QrTAE4...</td>\n",
              "      <td>Sun Sep 23 05:20:22 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042230772919357441</th>\n",
              "      <td>OBS: espero do fundo do meu coração que TODOS ...</td>\n",
              "      <td>Wed Sep 19 01:55:54 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036935048950435841</th>\n",
              "      <td>@CardosoTarciana Foi tão de última hora amigo ...</td>\n",
              "      <td>Tue Sep 04 11:12:35 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031711642567041025</th>\n",
              "      <td>@sinistra96 Tá igual a mim :(</td>\n",
              "      <td>Tue Aug 21 01:16:38 +0000 2018</td>\n",
              "      <td>Negativo</td>\n",
              "      <td>:(</td>\n",
              "      <td>no_theme-Negativo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>668853 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                            tweet_text  ...               type\n",
              "id                                                                      ...                   \n",
              "1047327129678110721  @afabianepereira além disso, o macro sempre va...  ...  no_theme-Positivo\n",
              "1038220551863627777                              aaaaaaa firmino :))))  ...  no_theme-Positivo\n",
              "1042927414794440704       @smtferreira não me chateia desse jeito :(((  ...  no_theme-Negativo\n",
              "1036605417290383360               @ddlnopromises adeus meta fitness :(  ...  no_theme-Negativo\n",
              "1042021132210200576  @mateus_brando8 @percosk Vc disse \"generaliza ...  ...  no_theme-Positivo\n",
              "...                                                                ...  ...                ...\n",
              "1044415947983998977  eu te amo muito muito muito :( https://t.co/um...  ...  no_theme-Negativo\n",
              "1043731778488406016  poxa nao estou na lista :( https://t.co/QrTAE4...  ...  no_theme-Negativo\n",
              "1042230772919357441  OBS: espero do fundo do meu coração que TODOS ...  ...  no_theme-Negativo\n",
              "1036935048950435841  @CardosoTarciana Foi tão de última hora amigo ...  ...  no_theme-Negativo\n",
              "1031711642567041025                      @sinistra96 Tá igual a mim :(  ...  no_theme-Negativo\n",
              "\n",
              "[668853 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZgvI5J3r3AA",
        "outputId": "761148e4-c3cc-4059-bdfa-889627256450"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "# from nltk.tokenize import sent_tokenize (Tokenization)\n",
        "from nltk.probability import FreqDist\n",
        "def _remove_url(data):\n",
        "    ls = []\n",
        "    words = ''\n",
        "    regexp1 = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    regexp2 = re.compile('www?.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    \n",
        "    for line in data:\n",
        "        urls = regexp1.findall(line)\n",
        "\n",
        "        for u in urls:\n",
        "            line = line.replace(u, ' ')\n",
        "\n",
        "        urls = regexp2.findall(line)\n",
        "\n",
        "        for u in urls:\n",
        "            line = line.replace(u, ' ')\n",
        "            \n",
        "        ls.append(line)\n",
        "    return ls\n",
        "\n",
        "def _remove_regex(data, regex_pattern):\n",
        "  ls = []\n",
        "  words = ''\n",
        "  \n",
        "  for line in data:\n",
        "      matches = re.finditer(regex_pattern, line)\n",
        "      \n",
        "      for m in matches: \n",
        "          line = re.sub(m.group().strip(), '', line)\n",
        "\n",
        "      ls.append(line)\n",
        "\n",
        "  return ls\n",
        "def _replace_emoticons(data, emoticon_list):\n",
        "    ls = []\n",
        "\n",
        "    for line in data:\n",
        "        for exp in emoticon_list:\n",
        "            line = line.replace(exp, \"\")\n",
        "\n",
        "        ls.append(line)\n",
        "\n",
        "    return ls\n",
        "\n",
        "def _tokenize_text(data):\n",
        "    ls = []\n",
        "\n",
        "    for line in data:\n",
        "        tokens = wordpunct_tokenize(line)\n",
        "        ls.append(tokens)\n",
        "\n",
        "    return ls\n",
        "\n",
        "def _remove_stopwords(tokens, stopword_list):\n",
        "    ls = []\n",
        "\n",
        "    for tk_line in tokens:\n",
        "        new_tokens = []\n",
        "        \n",
        "        for word in tk_line:\n",
        "            if word.lower() not in stopword_list:\n",
        "                new_tokens.append(word) \n",
        "            \n",
        "        ls.append(\" \".join(new_tokens))\n",
        "        \n",
        "    return ls\n",
        "\n",
        "\n",
        "def _apply_standardization(tokens, std_list):\n",
        "    ls = []\n",
        "\n",
        "    for tk_line in tokens:\n",
        "        new_tokens = []\n",
        "        \n",
        "        for word in tk_line:\n",
        "            if word.lower() in std_list:\n",
        "                word = std_list[word.lower()]\n",
        "                \n",
        "            new_tokens.append(word) \n",
        "            \n",
        "        ls.append(new_tokens)\n",
        "\n",
        "    return ls\n",
        "\n",
        "# get nltk portuguese stopwords\n",
        "nltk_stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "# notations\n",
        "regex_pattern = '@[\\w]*'\n",
        "xTrain = np.array(train[\"tweet_text\"])\n",
        "yTrain = np.array(train[\"sentiment\"])\n",
        "xTest = np.array(test[\"tweet_text\"])\n",
        "yTest = np.array(test[\"sentiment\"])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21GnP5Uczcp8"
      },
      "source": [
        "emoticon_list = {':))': 'positive_emoticon', ':)': 'positive_emoticon', ':D': 'positive_emoticon', ':(': 'negative_emoticon', ':((': 'negative_emoticon', '8)': 'neutral_emoticon'}\n",
        "std_list = {'eh': 'é', 'vc': 'você', 'vcs': 'vocês','tb': 'também', 'tbm': 'também', 'obg': 'obrigado', 'gnt': 'gente', 'q': 'que', 'n': 'não', 'cmg': 'comigo', 'p': 'para', 'ta': 'está', 'to': 'estou', 'vdd': 'verdade'}\n",
        "xTrain = _remove_url(xTrain)\n",
        "xTest  = _remove_url(xTest)\n",
        "xTrain = _remove_regex(xTrain, regex_pattern)\n",
        "xTest  = _remove_regex(xTest, regex_pattern)\n",
        "xTrain = _replace_emoticons(xTrain, emoticon_list)\n",
        "xTest  = _replace_emoticons(xTest, emoticon_list)\n",
        "xTrain = _tokenize_text(xTrain)\n",
        "xTest  = _tokenize_text(xTest)\n",
        "xTrain = _apply_standardization(xTrain, std_list)\n",
        "xTest  = _apply_standardization(xTest, std_list)\n",
        "xTrain = _remove_stopwords(xTrain, nltk_stopwords)\n",
        "xTest = _remove_stopwords(xTest, nltk_stopwords)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYovhSN94tAW"
      },
      "source": [
        ""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_iJM-B5MIR"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX5aqEdWxSWF"
      },
      "source": [
        "\n",
        "dataTrain = pd.DataFrame({'X': xTrain, 'Y': yTrain})\n",
        "dataTest  = pd.DataFrame({'X': xTest, 'Y': yTest})\n",
        "\n",
        "dfTotal = pd.concat([dataTrain, dataTest], axis=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5KPERccxRiU"
      },
      "source": [
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/corpus.txt\",\"w+\")\n",
        "i=1\n",
        "for row in dfTotal.iterrows():\n",
        "  try:\n",
        "    allFile.write(row[1][\"X\"]+\"\\n\")\n",
        "    i+= 1\n",
        "  except:\n",
        "    print(\"Rcorded lines: \", i)\n",
        "    print(dfTotal.shape)\n",
        "\n",
        "\n",
        "allFile.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwhTMheotDJ1"
      },
      "source": [
        "dfAgainst  = dataTrain[dataTrain[\"Y\"] == \"Negativo\"]\n",
        "dfInfavor  = dataTrain[dataTrain[\"Y\"] == \"Positivo\"]\n",
        "\n",
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/sentiment.train.1\",\"w+\")\n",
        "\n",
        "for row in dfInfavor.iterrows():\n",
        "  allFile.write(row[1][\"X\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()\n",
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/sentiment.train.0\",\"w+\")\n",
        "\n",
        "for row in dfAgainst.iterrows():\n",
        "  allFile.write(row[1][\"X\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2GqxjlQ-uw9"
      },
      "source": [
        "dfAgainst  = dataTest[dataTest[\"Y\"] == \"Negativo\"]\n",
        "dfInfavor  = dataTest[dataTest[\"Y\"] == \"Positivo\"]\n",
        "\n",
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/sentiment.test.1\",\"w+\")\n",
        "\n",
        "for row in dfInfavor.iterrows():\n",
        "  allFile.write(row[1][\"X\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()\n",
        "\n",
        "#read data dataset and store\n",
        "allFile = open(r\"/content/drive/MyDrive/sentimentTranslation/data/sentiment.test.0\",\"w+\")\n",
        "\n",
        "for row in dfAgainst.iterrows():\n",
        "  allFile.write(row[1][\"X\"]+\"\\n\")\n",
        "  \n",
        "allFile.close()"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}